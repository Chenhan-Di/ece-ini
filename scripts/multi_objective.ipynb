{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad368988-5f0a-4a07-89ee-9a8414ab0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0820b35-dd78-4e16-857a-4a280c736c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from botorch.models.transforms import Normalize\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from gpytorch.priors import GammaPrior\n",
    "from gpytorch.kernels import ScaleKernel,RBFKernel,AdditiveKernel,MaternKernel\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import (\n",
    "    FastNondominatedPartitioning,\n",
    ")\n",
    "from  botorch.acquisition.multi_objective.logei import qLogExpectedHypervolumeImprovement\n",
    "from botorch.utils.multi_objective.box_decompositions.dominated import (\n",
    "    DominatedPartitioning,\n",
    ")\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926a9b7-bac0-4d85-ba30-b4e544dede0e",
   "metadata": {},
   "source": [
    "## ===============================GP Training====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acd4b8-51c3-41b7-9964-406b1eddfb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "json_file = \"pism_rmse.json\"\n",
    "with open(json_file, \"r\") as f:\n",
    "    log_data = json.load(f)\n",
    "\n",
    "# Extract Params\n",
    "q = [entry[\"q\"] for entry in log_data]\n",
    "pmin = [entry[\"pmin\"] for entry in log_data]\n",
    "pmax = [entry[\"pmax\"] for entry in log_data]\n",
    "zmin = [entry[\"zmin\"] for entry in log_data]\n",
    "zmax = [entry[\"zmax\"] for entry in log_data]\n",
    "lapse = [entry[\"lapse\"] for entry in log_data]\n",
    "essa = [entry[\"essa\"] for entry in log_data]\n",
    "esia = [entry[\"esia\"] for entry in log_data]\n",
    "\n",
    "# Extract Target\n",
    "rmse_thk = [entry[\"thkRMSE\"] for entry in log_data]\n",
    "rmse_vel = [entry[\"velmagRMSE\"] for entry in log_data]\n",
    "\n",
    "# Transform Parameters and Target\n",
    "rmse_thk_r = torch.tensor(rmse_thk,dtype=torch.double).view(-1, 1)  # Reshape to (n, 1)\n",
    "rmse_vel_r = torch.tensor(rmse_vel,dtype=torch.double).view(-1, 1) \n",
    "\n",
    "# Combine parameters into a list of lists for processing\n",
    "params = list(zip(q, pmin, pmax, zmin, zmax, lapse, essa, esia))\n",
    "params = torch.tensor([list(p) for p in params], dtype=torch.double) \n",
    "\n",
    "# Set params and rmse for multi-objective GP\n",
    "params_multi=torch.cat([params,params])\n",
    "rmse_mult=torch.cat([-rmse_thk_r,-rmse_vel_r],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1de3a39-b1a2-4b81-9a6a-70fc44a51eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The range of parameters (q, phi_min, phi_max, z_min, z_max, gamma_M, e_ssa, e_sia)\n",
    "bounds_parameters = torch.tensor([\n",
    "            [0,1],\n",
    "            [0,15],\n",
    "            [35,46],\n",
    "            [-1000,0],\n",
    "            [0,1000],\n",
    "            [0,5],\n",
    "            [0.1,2],\n",
    "            [1,5],\n",
    "        ],dtype=torch.double).T\n",
    "\n",
    "def initialize_model(train_x, train_obj):\n",
    "    bounds = bounds_parameters\n",
    "    models = []\n",
    "    #Ice Surface Elevation GP\n",
    "    train_y = train_obj[..., 0:1]\n",
    "    models.append(\n",
    "        SimpleEGP(\n",
    "            train_x, train_y\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #Ice Surface Velocity GP\n",
    "    train_y = train_obj[..., 1:2]\n",
    "    models.append(\n",
    "        SimpleVGP(\n",
    "            train_x, train_y\n",
    "        )\n",
    "    )\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a8354-2272-455f-90d8-abfd0cfa0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Kernel\n",
    "class SimpleEGP(SingleTaskGP):\n",
    "    def __init__(self, train_X, train_Y):\n",
    "        bound = bounds_parameters\n",
    "        \n",
    "        super().__init__(train_X, train_Y, input_transform=Normalize(d=8,bounds=bound), outcome_transform=Standardize(m=1))\n",
    "\n",
    "        # Create RBF kernel for dims except phi_max and e_ssa\n",
    "        rbf_dims = [0, 1, 3, 4, 5, 7]\n",
    "        kernel_rbf = RBFKernel(ard_num_dims=len(rbf_dims), active_dims=torch.tensor(rbf_dims))\n",
    "        \n",
    "        # Create Matern kernel for phi_max and e_ssa\n",
    "        matern_dims = [2,6]\n",
    "        kernel_matern = MaternKernel(nu=1.5, ard_num_dims=len(matern_dims), active_dims=torch.tensor(matern_dims))\n",
    "        \n",
    "        # Register prior only on e_sia\n",
    "        kernel_rbf.register_prior(\n",
    "            \"lengthscale_prior_last_dim\", GammaPrior(2, 6),\n",
    "            lambda m: m.lengthscale[..., -1]\n",
    "        )\n",
    "\n",
    "        # Combine kernels using additive kernel\n",
    "        mixed_kernel = AdditiveKernel(kernel_rbf, kernel_matern)\n",
    "\n",
    "        # Wrap in ScaleKernel\n",
    "        self.covar_module = ScaleKernel(mixed_kernel)\n",
    "\n",
    "# Kernel for velocity\n",
    "class SimpleVGP(SingleTaskGP):\n",
    "    def __init__(self, train_X, train_Y):\n",
    "        bound = bounds_parameters\n",
    "        \n",
    "        super().__init__(train_X, train_Y, input_transform=Normalize(d=8,bounds=bound), outcome_transform=Standardize(m=1))\n",
    "\n",
    "        # Create RBF kernel for e_sia\n",
    "        rbf_dims = [7]\n",
    "        kernel_rbf = RBFKernel(ard_num_dims=len(rbf_dims), active_dims=torch.tensor(rbf_dims))\n",
    "\n",
    "        # Create Matern kernel for other dims\n",
    "        matern2_dims = [0, 1, 3, 4, 5]\n",
    "        kernel_matern2 = MaternKernel(nu=2.5, ard_num_dims=len(matern2_dims), active_dims=torch.tensor(matern2_dims))\n",
    "        \n",
    "        # Create Matern kernel for phi_max and e_ssa\n",
    "        matern_dims = [2,6]\n",
    "        kernel_matern = MaternKernel(nu=1.5, ard_num_dims=len(matern_dims), active_dims=torch.tensor(matern_dims))\n",
    "\n",
    "        # Combine kernels using additive kernel\n",
    "        mixed_kernel = AdditiveKernel(kernel_rbf, kernel_matern2, kernel_matern)\n",
    "\n",
    "        # Wrap in ScaleKernel\n",
    "        self.covar_module = ScaleKernel(mixed_kernel)\n",
    "\n",
    "d=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f6b41-5d73-421b-a09c-8cfc58eb787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GP\n",
    "mll, model = initialize_model(params, rmse_mult)\n",
    "fit_gpytorch_mll(mll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9333f03-4623-4822-a2a0-2daff8f1151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the model output on training data\n",
    "a=model.train_inputs[0]\n",
    "model_output = model(a[0],a[0])\n",
    "\n",
    "# Compute the marginal log-likelihood\n",
    "mll_value = mll(model_output, model.train_targets).item()\n",
    "\n",
    "print(\"Marginal Log-Likelihood:\", mll_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0c783-53d8-4d31-9264-51bea79c4f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the lengthscale for ice surface elevation\n",
    "print(model.models[0].covar_module.base_kernel.kernels[0].lengthscale.detach())\n",
    "print(model.models[0].covar_module.base_kernel.kernels[1].lengthscale.detach())\n",
    "\n",
    "# Print the lengthscale for ice surface velocity magnitude\n",
    "print(model.models[1].covar_module.base_kernel.kernels[0].lengthscale.detach())\n",
    "print(model.models[1].covar_module.base_kernel.kernels[1].lengthscale.detach())\n",
    "print(model.models[1].covar_module.base_kernel.kernels[2].lengthscale.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd544d8-6249-4f4b-97bc-a14905a6e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidates size\n",
    "BATCH_SIZE = 4\n",
    "NUM_RESTARTS = 15\n",
    "RAW_SAMPLES = 512\n",
    "\n",
    "standard_bounds = torch.zeros(2, 7)\n",
    "standard_bounds[1] = 1\n",
    "standard_bounds = bounds_parameters\n",
    "ref_point=torch.tensor([-360,-360],dtype=torch.float64)\n",
    "\n",
    "def optimize_qehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(train_x).mean\n",
    "    partitioning = FastNondominatedPartitioning(\n",
    "        ref_point=ref_point,\n",
    "        Y=pred,\n",
    "    )\n",
    "    acq_func = qLogExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=ref_point,\n",
    "        partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    \n",
    "    # observe new values\n",
    "    new_x = candidates\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37de04-cdcc-4e85-94dd-d3917637ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the qehvi acquisition modules using a QMC sampler\n",
    "qehvi_sampler = SobolQMCNormalSampler(sample_shape=torch.Size([128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941270fa-b8cf-4989-b0d6-8bca216a037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hypervolume\n",
    "bd = DominatedPartitioning(ref_point=ref_point, Y=rmse_mult)\n",
    "volume = bd.compute_hypervolume().item()\n",
    "print(volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c140f-588f-4175-b701-9cbeb003c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next generation candidate\n",
    "candidate = optimize_qehvi_and_get_observation(model, params, rmse_mult, qehvi_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acfbef-18df-4344-9f15-6fe0e48dd884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the predicted RMSE of the candidate\n",
    "print(-model.posterior(candidate).mean)\n",
    "print(model.posterior(candidate).variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500dd6d-09d9-45e6-bbb9-7c835b963e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Candidate\n",
    "print(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ba442-6042-42d5-9e05-3b13ec1caf4e",
   "metadata": {},
   "source": [
    "## ======================= Bayesian Calibration ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a2d86-1309-46a8-a2de-e5d069485ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_model(mean,variance,option):\n",
    "    # Normalize & Standarize\n",
    "    mean_cal = mean[0,option].detach().numpy()\n",
    "    variance_cal = variance[0,option].detach().numpy()\n",
    "    return [(mean_cal-transformmean[option])/transformstd[option],variance_cal/transformstd[option]**2]\n",
    "\n",
    "def logll(x_torch,n):\n",
    "    # Calculate the log-likelihood\n",
    "\n",
    "    # The posterior in GP\n",
    "    posterior = model.posterior(x_torch)\n",
    "    mean, variance = posterior.mean, posterior.variance\n",
    "    \n",
    "    # Ice surface elevation\n",
    "    mean_cal,variance_cal=transform_model(mean,variance,0)\n",
    "    likelihood1 = -0.5*np.log(2*np.pi*variance_cal)-mean_cal**2/(2*variance_cal)\n",
    "    \n",
    "    # Ice surface velocity magnitude\n",
    "    mean_cal,variance_cal = transform_model(mean,variance,1)\n",
    "    likelihood2 = -0.5*np.log(2*np.pi*variance_cal)-mean_cal**2/(2*variance_cal)\n",
    "\n",
    "    # Return weighted mean\n",
    "    return n*likelihood1+(1-n)*likelihood2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914202bd-7c29-440f-87d9-65169f3c6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the transform in GP\n",
    "transform1=model.models[0].input_transform\n",
    "transformmean=[model.models[0].outcome_transform.means.detach().numpy()[0,0],model.models[1].outcome_transform.means.detach().numpy()[0,0]]\n",
    "transformstd=[model.models[0].outcome_transform.stdvs.detach().numpy()[0,0],model.models[1].outcome_transform.stdvs.detach().numpy()[0,0]]\n",
    "\n",
    "# Start Point\n",
    "x=np.array([[1.0000,6.4074,46.0000,-362.5400,0.0000,2.5183,1.1229,1.5745]])\n",
    "torch_x=torch.tensor(x, dtype=torch.double)\n",
    "posterior = model.posterior(torch_x)  # Predict for candidates\n",
    "mean_start, variance_mean_start = posterior.mean, posterior.variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ebc41-d526-4845-8061-f81b41c63b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC\n",
    "\n",
    "n = 100000 # Number\n",
    "x_examples = np.random.rand(n+1, 8) # Store\n",
    "scale = 0.2 # Neiborhood\n",
    "x_examples[0,:] = x\n",
    "\n",
    "# Control the weight of log-likelihood (0-velocity calibration, 1-elevation calibration)\n",
    "weight = 1\n",
    "\n",
    "for i in range(n):\n",
    "    x_old = transform1(torch_x).detach().numpy()\n",
    "    \n",
    "    k = np.array([np.random.normal(loc=0.0, scale=scale),np.random.normal(loc=0.0, scale=scale),np.random.normal(loc=0.0, scale=scale),np.random.normal(loc=0.0, scale=scale),np.random.normal(loc=0.0, scale=scale),np.random.normal(loc=0.0, scale=scale),np.random.normal(loc=0.0, scale=scale),np.random.normal(loc=0.0, scale=scale)])\n",
    "    x_new = x_old+k\n",
    "\n",
    "    # Reflection\n",
    "    x_ref = np.where(x_new[0]<0,-x_new[0],x_new[0])\n",
    "    x_ref = np.where(x_new[0]>1,2-x_new[0],x_new[0])\n",
    "    \n",
    "    # Avoid little outside bound\n",
    "    x_new[0] = np.clip(x_ref,0,1)\n",
    "    \n",
    "    # Data Pre-process\n",
    "    x_old_c = transform1.untransform(torch.tensor(x_old, dtype=torch.double))\n",
    "    x_new_c = transform1.untransform(torch.tensor(x_new, dtype=torch.double))\n",
    "    \n",
    "    # Metropolis Rule\n",
    "    \n",
    "    p = np.random.rand(1)\n",
    "    \n",
    "    # Accept\n",
    "    if logll(x_torch=x_new_c,n=weight)>logll(x_torch=x_old_c,n=weight):\n",
    "        x_old = x_new\n",
    "        x_old_store = x_new_c.detach().numpy()\n",
    "    elif np.exp(logll(x_torch=x_new_c,n=weight)-logll(x_torch=x_old_c,n=weight))<p:\n",
    "        x_old = x_new\n",
    "        x_old_store = x_new_c.detach().numpy()\n",
    "    # Reject\n",
    "    else:\n",
    "        x_old = x_old\n",
    "        x_old_store = x_old_c.detach().numpy()\n",
    "\n",
    "    # Store\n",
    "    x_examples[i+1,:] = x_old_store\n",
    "\n",
    "    if np.mod(i,10000)==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce1962-0f36-4c48-af91-bbe03fd9081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of dimensions to plot\n",
    "num_dims = 8\n",
    "bins = 50\n",
    "title = [\"q\",\"phi_min\",\"phi_max\",\"z_min\",\"z_max\",\"gamma_M\",\"e_ssa\",\"e_sia\"]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "axes = axes.flatten()\n",
    "xlims = [(0, 1), (0, 15), (35, 46), (-1000, 0), (0, 1000), (0, 5), (0, 2), (1, 5)]\n",
    "\n",
    "for dim_index in range(num_dims):\n",
    "    values = x_examples[:, dim_index]\n",
    "    \n",
    "    # Plot histogram\n",
    "    sns.histplot(values, bins=bins, kde=True, alpha=0.5, stat='probability',ax=axes[dim_index],color='dimgray')\n",
    "    \n",
    "    # Set title for each subplot\n",
    "    axes[dim_index].set_title(f'{title[dim_index]}')\n",
    "    \n",
    "# Show and Save\n",
    "plt.tight_layout()\n",
    "#plt.savefig('count_both.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
